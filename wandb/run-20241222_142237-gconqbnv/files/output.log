Namespace(root_path='kits19/data', list_dir='./lists_kits19', dataset='KiTS19', num_classes=3, max_iterations=50000, max_epochs=150, batch_size=4, n_gpu=1, deterministic=1, base_lr=0.01, img_size=224, seed=1234, n_skip=3, vit_name='R50-ViT-B_16', vit_patches_size=16, checkpoint_dir='./checkpoints_kits1', use_attention=1, exp='TU_KiTS19_224')
No checkpoint or best model found. Starting training from scratch.
18375 iterations per epoch. 50000 total iterations.
  0%|                                       | 0/50000 [00:00<?, ?it/s]
[DecoderCup] Before conv_more: x.shape=torch.Size([4, 768, 14, 14])
[DecoderCup] After conv_more: x.shape=torch.Size([4, 512, 14, 14])
[DecoderCup] Skip connection 0: skip.shape=torch.Size([4, 512, 28, 28])
[DecoderCup] After block 0: x.shape=torch.Size([4, 256, 28, 28])
[DecoderCup] Skip connection 1: skip.shape=torch.Size([4, 256, 56, 56])
[DecoderCup] After block 1: x.shape=torch.Size([4, 128, 56, 56])
[DecoderCup] Skip connection 2: skip.shape=torch.Size([4, 64, 112, 112])
[DecoderCup] After block 2: x.shape=torch.Size([4, 64, 112, 112])
  0%|                                       | 0/50000 [00:21<?, ?it/s]
Traceback (most recent call last):
  File "/home/hous/Desktop/Kidney_Segmentation/TransUNet/train.py", line 86, in <module>
    trainer_kits19(args, net)
  File "/home/hous/Desktop/Kidney_Segmentation/TransUNet/trainer.py", line 142, in trainer_kits19
    loss.backward()
  File "/home/hous/anaconda3/envs/torch/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/hous/anaconda3/envs/torch/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [4, 32, 112, 112]], which is output 0 of ReluBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).